{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction & Motivation\n",
    "\n",
    "Topic models take a corpus of human-readable text documents and cluster them.  The idea is that documents are assigned to the same cluster when they are \"about\" the same topic.  The field has advanced a long way since the introduction of _Latent Dirichlet Allocation_ back in 2003.  These days deep learning topic models are in vogue.  In particular, BERT-based topic models are appealing because they can naturally handle out-of-vocabulary tokens and generate powerful text embeddings.\n",
    "\n",
    "This notebook is a stripped-down version of something I created for finding granular topics in short-form social media texts.  I had been experimenting with many published models - including Bayesian ones, non-negative matrix factrisation methods and deep-learning based ones but was not getting the results I wanted.  My particular use case required low-level, granular topics - essentially I wanted to define a topic as a particular variation of a claim, an event or a specifc belief.  Everything I tried was either returning broader topics (\"COVID 19\", \"war in ukraine\", etc) - or it was totally failing to get to grips with the truncated nature of short-form posts (like tweets).\n",
    "\n",
    "I took inspiration from Dimo Angelov's **top2vec** paper (https://arxiv.org/abs/2008.09470).  The author bolted together an embedding layer - which generated 768-dimensional, dense vectors - with UMAP to reduce said dimensionality and finally HDBScan to yield a partial clustering of the documents.  The centroid of the found clusters could then be reprojected into the embedding space, yielding a joint embedding of documents and topics.\n",
    "\n",
    "Lovely though the process is I couldn't achieve good results on my short-form data at the level of granularity I wanted.  So I have adapted Dimo's approach, and it works really nicely.  What follows is a first step towards a model that clusters tweets at the level of a particular claim, statistic or event.  (This is not about identifying or evaluating claims and opinions _per se_.)\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "1. Properly prepare the tweets, removing URLS and hashtags (which tend to be unhelpful when they appear in groups).\n",
    "2. Fine-tune a BERT model using SimCSE - a form of Multiple Negative-Ranking Loss.  This yields an embedding tuned to the specific domains of the data I am working with.\n",
    "3. Use the fine-tuned model to embed the tweets.\n",
    "4. Build an (approximate) neareset-neighbours index over the embedded tweets.  This enables us to efficiently find similar tweets without having to make $\\mathcal{O}\\left(N^2\\right)$ comparisons.\n",
    "5. Build a graph where each tweet is a node and each nearest neighbour (above a similarity threshold) is an edge.\n",
    "6. Run a simple community detection algorithm over the graph.\n",
    "\n",
    "Voila: topics.\n",
    "\n",
    "### A Word on Data\n",
    "\n",
    "This notebook uses a set of tweets, collected in 2022 with keywords that relate to the cost of living crisis and the then Prime Minister's alleged breaches of the UK's COVID-19 lockdown rules.  Twitter's terms of use do not allow for their data to be shared, but this notebook will work on any set of tweets you may have to hand (I've tested it against lots of them!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Will\\Anaconda3\\envs\\deepseer3\\lib\\site-packages\\huggingface_hub\\snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'graph_tool', 'wurlitzer', 'karateclub', 'leidenalg'}\n",
      "Note: to be able to use all overlapping methods, you need to install some additional packages:  {'ASLPAw', 'karateclub'}\n",
      "Note: to be able to use all bipartite methods, you need to install some additional packages:  {'wurlitzer', 'leidenalg'}\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "from textacy import preprocessing\n",
    "from functools import partial\n",
    "from sentence_transformers import SentenceTransformer, InputExample, models, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.similarities.annoy import AnnoyIndexer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from networkx import NetworkXError\n",
    "from cdlib.algorithms import label_propagation\n",
    "from collections import namedtuple, Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20,10)})\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42                 # For repeatable experiments\n",
    "REBUILD = True                   # Set to TRUE to force a rebuild of the core transformer, embeddings, Annoy index and sim-graph\n",
    "\n",
    "# Data extraction\n",
    "PROJECT_SAMPLE_SIZE = 100000     # The number of source tweets we'll retrieve\n",
    "\n",
    "# Pre-processing \n",
    "MAX_HASHTAGS_BEFORE_REMOVAL = 0  # If there are more than this many hashtags in a tweet, remove them all.\n",
    "\n",
    "# Dictionary-building\n",
    "MIN_TOKEN_COUNT = 20             # Minimum # token occurances for a dictionary entry\n",
    "MAX_TOKEN_FREQ = 0.25            # When building the token dictionary, any token appearing in more than this % of documents is pruned\n",
    "\n",
    "# Fine-tuning\n",
    "N_TRAINING_EPOCHS = 2            # Number of epochs of fine-tuning for the transformer.\n",
    "MAX_SEQ_LEN = 100                # Large enough for the vast majority of tweets.\n",
    "\n",
    "# Indexing\n",
    "NUM_ANNOY_TREES = 1              # How many NN trees to build.  More trees = more accurate recall.\n",
    "\n",
    "# Topic Graph\n",
    "MIN_SIM_SCORE = 0.45             # Minimum similarity between embedding vectors to yield an edge in the topic graph.\n",
    "MIN_CLUSTER_SIZE = 20            # Discard topic clusters with fewer than this number of members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Minimally clean the data, making it suitable for BERT finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains [100000] rows.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"./data/data.csv\").dropna()\n",
    "print(f\"Dataset contains [{df.shape[0]}] rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip URLs\n",
    "\n",
    "First find all of the t.co URLs and place them into a new column.  Remove them from the tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS_RE = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "\n",
    "df['urls'] = df.tweet_text.apply(URLS_RE.findall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the URLs from the tweet text\n",
    "\n",
    "def remove_urls(row):\n",
    "    tweet_text = row.tweet_text\n",
    "    for u in row.urls:\n",
    "         tweet_text = tweet_text.replace(u, '')\n",
    "    return tweet_text.strip()\n",
    "\n",
    "df.tweet_text = df.apply(remove_urls, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Text Preprocessing\n",
    "\n",
    "The only point here of note is the handling of hashtags.  Tweets often contain a large number of hashtags appended to the main body of the text.  I've found these actually _detract_ from the ability of topic models to do their work properly.  So in the pre-processing code below, if there are more than `MAX_HASHTAGS_BEFORE_REMOVAL` hashtags then _all_ of them will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_user_handles = partial(preprocessing.replace.user_handles, repl='')\n",
    "replace_urls = partial(preprocessing.replace.urls, repl='')\n",
    "replace_hashtags = partial(preprocessing.replace.hashtags, repl='')\n",
    "remove_ampersand = lambda text: text.replace('&amp', '')\n",
    "remove_newline = lambda text: text.replace('\\n', ' ')\n",
    "replace_emojis = partial(preprocessing.replace.emojis, repl='')\n",
    "delete_chars = lambda text: text.replace(\"'\", \"\").replace(\"-\", \"\")\n",
    "\n",
    "preproc = preprocessing.make_pipeline(\n",
    "    preprocessing.normalize.bullet_points,\n",
    "    preprocessing.normalize.quotation_marks,\n",
    "    preprocessing.normalize.whitespace,\n",
    "    preprocessing.normalize.unicode,\n",
    "    preprocessing.remove.accents,\n",
    "    replace_urls,\n",
    "    remove_ampersand,\n",
    "    delete_chars,    \n",
    "    replace_user_handles,\n",
    "    preprocessing.remove.punctuation,    \n",
    "    replace_emojis,\n",
    "    remove_newline,\n",
    "    preprocessing.remove.brackets,\n",
    "    preprocessing.normalize.whitespace\n",
    ")\n",
    "\n",
    "preproc_no_hashtags = preprocessing.make_pipeline(\n",
    "    preprocessing.normalize.bullet_points,\n",
    "    preprocessing.normalize.quotation_marks,\n",
    "    preprocessing.normalize.whitespace,\n",
    "    preprocessing.normalize.unicode,\n",
    "    preprocessing.remove.accents,\n",
    "    replace_urls,\n",
    "    remove_ampersand,    \n",
    "    # Replace hashtags with the empty string before we handle punctuation\n",
    "    replace_hashtags,\n",
    "    replace_user_handles,\n",
    "    delete_chars,\n",
    "    preprocessing.remove.punctuation,\n",
    "    replace_emojis,\n",
    "    remove_newline,\n",
    "    preprocessing.remove.brackets,\n",
    "    preprocessing.normalize.whitespace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, hashtag_threshold=MAX_HASHTAGS_BEFORE_REMOVAL):\n",
    "    if text.count(\"#\") > hashtag_threshold:\n",
    "        return preproc_no_hashtags(text)\n",
    "    else:\n",
    "        return preproc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = [preprocess(t) for t in df.tweet_text.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the SimCSE transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimCSE model (https://arxiv.org/abs/2104.08821) works as follows:\n",
    "\n",
    "1. Start with a pretrained BERT transformer model to obtain fixed-dimensional word embeddings.  We use DistilRoBERTa (https://huggingface.co/docs/transformers/model_doc/distilbert) as the base. This is based on the RoBERTa model which only trains against the Masked Language Model (MLM) objective.  Dropout is set to 0.1 as standard.\n",
    "2. Stack a mean-pooling layer on top.  (The `all-distilroberta-v1` model was trained using mean-pooling, not the CLS token, to generate document embeddings.)\n",
    "3. Training examples consist of pairs of the same document (dropout masks are be applied separately, and randomly, to each item in a pair).\n",
    "4. Use the Multiple Negatives Ranking (MNR) Loss.  For example $x_i$ in a batch of $K$ input pairs the loss function evaluates:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{l}_i &= - \\text{log } \\mathbf{P}\\left(x_i | x_i\\right) \\\\\n",
    "& \\approx -\\text{log } \\left( \\frac{e^{\\text{sim}\\left(z_i^{\\left(1\\right)}, z_i^{\\left(2\\right)}\\right)}}{\\sum_{k=1}^{K}e^{\\text{sim}\\left(z_i^{\\left(1\\right)}, z_k^{\\left(2\\right)}\\right)}} \\right) \\\\\n",
    "&\\approx -\\text{sim}\\left(z_i^{\\left(1\\right)}, z_i^{\\left(2\\right)}\\right) + \\text{log } \\left(\\sum_{k=1}^{K}e^{\\text{sim}\\left(z_i^{\\left(1\\right)}, z_k^{\\left(2\\right)}\\right)} \\right)\n",
    "\\end{align}\n",
    "Where $\\text{sim}$ is the cosine similarity function, $z_i$ is the mean-embedding vector for example $x_i$ and the superscripts $z_i^{\\left(1\\right)}$ and $z_i^{\\left(2\\right)}$ denote the dropout masks applied to the first and second elements of the input pair, respectively.  (MNR loss is therefore effectively just a form of cross-entropy loss.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def finetune_simcse():\n",
    "\n",
    "    # Cased version of DistilRoBERTa.\n",
    "    model_name = 'sentence-transformers/all-distilroberta-v1'\n",
    "\n",
    "    word_embedding_model = models.Transformer(model_name, max_seq_length=32)\n",
    "\n",
    "    pooling_model = models.Pooling(\n",
    "        word_embedding_dimension=word_embedding_model.get_word_embedding_dimension(),\n",
    "        pooling_mode='mean'\n",
    "    )\n",
    "\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    # SimCSE uses masked pairs of inputs.\n",
    "    train_data = [InputExample(texts=[doc, doc]) for doc in df.preprocessed_text.to_numpy()]\n",
    "\n",
    "    # Batch the data.  Larger batches make the MNR loss more accurate.\n",
    "    # Shuffling super-important, otherwise the MNR loss will not work \n",
    "    # (because we risk near identical tweets appearing next to each other in the dataset).\n",
    "    train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "    \n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=N_TRAINING_EPOCHS,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    model.save(f\"./transformers/model\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning using SimCSE task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cf1e606d944fe49cdb95fdae121e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3df052845464d6b8f6c663bd2c83159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b75ccc37e82409a95c1104801e96162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load or finetune a SimCSE model\n",
    "\n",
    "try:\n",
    "    if REBUILD:\n",
    "        raise ValueError()\n",
    "    else:\n",
    "        print(\"Loading transformer model\")\n",
    "        model = SentenceTransformer(f\"./transformers/model\")\n",
    "except ValueError:\n",
    "    print(\"Fine-tuning using SimCSE task\")    \n",
    "    model = finetune_simcse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate the embeddings\n",
    "\n",
    "Run the model in execution mode (no dropouts) to generate embedded vectors.  The vectors are wrapped in Gensim's `KeyedVectors` class and form inputs to the indexing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6740c4eae1bd43c59b228854b4ad9bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    if REBUILD:\n",
    "        raise FileNotFoundError()\n",
    "    else:\n",
    "        print(\"Loading Embedding\")        \n",
    "        embedding = KeyedVectors.load(f\"./embeddings/embedding__x768.kv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Generating Embedding\")\n",
    "    embedding = KeyedVectors(model[1].word_embedding_dimension)\n",
    "    embedding.add_vectors(\n",
    "        keys=range(n_docs),\n",
    "        weights=model.encode(\n",
    "            df.preprocessed_text.to_numpy(), \n",
    "            show_progress_bar=True, \n",
    "            convert_to_numpy=True, \n",
    "            device='cuda'\n",
    "        )\n",
    "    )\n",
    "    embedding.save(f\"./embeddings/embedding__x768.kv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search\n",
    "\n",
    "We proceed as follows:\n",
    "\n",
    "Construct an efficient index for performing similarity search over our embedding vectors.  I have chosen Approximate Nearest Neighbours (A-NN), implemented by the `annoy` package (https://github.com/spotify/annoy), which comes packaged with `gensim`.  It has the following properties:\n",
    "  1. It is stochastic (i.e. it doesn't guarantee the same - or a complete - similarity set each time you run it).  However, given subsequent steps this has minimal impact on the clustering results.\n",
    "  2. `annoy`, specifically, is fast - making it well suited to prototyping.\n",
    "  3. Nearest Neighbours, in general, is simple.  For implementing at scale we might want to compare A-NN with Locality Sensitive Hashing.\n",
    "  4. The default distance metric used by `annoy` is Cosine similarity, which is the same metric as was used during SimCSE training.  It'll also perform better than, say the Euclidean distance on a length 768 dense vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Annoy Index\n"
     ]
    }
   ],
   "source": [
    "annoy_index = AnnoyIndexer()\n",
    "\n",
    "try:\n",
    "    if REBUILD:\n",
    "        raise OSError()\n",
    "    else:\n",
    "        print(\"Loading Annoy Index\")\n",
    "        annoy_index.load(f\"./annoy_indexes/index.annoy\")\n",
    "except OSError:\n",
    "    print(\"Creating Annoy Index\")\n",
    "    annoy_index = AnnoyIndexer(embedding, num_trees=NUM_ANNOY_TREES)\n",
    "    annoy_index.save(f\"./annoy_indexes/index.annoy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory test: use the index to find examples of similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def most_similar_to_random_tweet(df, embedding, annoy_index, min_sim_score, min_matches=1, n_trials=100):\n",
    "    for i in range(n_trials):\n",
    "        doc_id = np.random.choice(df.shape[0])\n",
    "        vector = embedding.get_vector(doc_id)\n",
    "        dists = embedding.most_similar([vector], indexer=annoy_index)\n",
    "        dists = [(doc_id, score) for doc_id, score in dists if score >= min_sim_score and score < 1.0]\n",
    "        \n",
    "        if len(dists) >= min_matches:\n",
    "            print(colored(\"TEST TWEET\", \"red\", attrs=['bold', 'underline']))\n",
    "            print(colored(df.preprocessed_text.values[doc_id], attrs=['bold']))\n",
    "            print(colored(\"\\nMATCHES\", \"red\", attrs=['bold', 'underline']))\n",
    "            \n",
    "            for doc_id, score in dists:\n",
    "                doc_id = int(doc_id)\n",
    "                print(df.preprocessed_text.values[doc_id])\n",
    "                print(colored(f\"Cosine Similarity: {np.round(score, 2)}\\n\", \"green\"))\n",
    "            return None\n",
    "        \n",
    "    print(f\"{n_trials} tweets sampled but none had >= {min_matches} matches with score >= {min_sim_score}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[1m\u001b[31mTEST TWEET\u001b[0m\n",
      "\u001b[1mIntegrity What an absolute Joke They went on 24 7 trying to get Boris booted out and all the time they were doing worse Hypocrisy at the highest level By the way the whole of the UK can see straight through what Labour is trying to accomplish\u001b[0m\n",
      "\u001b[4m\u001b[1m\u001b[31m\n",
      "MATCHES\u001b[0m\n",
      "Not much integrity when he was exploiting grieving relatives against Boris Johnson in HoC knowing that he had been breaking the rules Hypocritical People see through all this\n",
      "\u001b[32mCosine Similarity: 0.47\n",
      "\u001b[0m\n",
      "Corrupt as fuck 7 times the Met came to Boris Johnsons rescue on Partygate\n",
      "\u001b[32mCosine Similarity: 0.46\n",
      "\u001b[0m\n",
      "Corrupt to the core the whole damn lot of them 7 times the Met came to Boris Johnsons rescue on Partygate\n",
      "\u001b[32mCosine Similarity: 0.45\n",
      "\u001b[0m\n",
      "Well Boris Johnsons govt was always going to be a nasty corrupt shambles Shame the Met Police have sunk so far But they consider it their job to prop up the rich and powerful Obviously\n",
      "\u001b[32mCosine Similarity: 0.43\n",
      "\u001b[0m\n",
      "Absolutely disgraceful The Liar Boris Johnson has escaped justice One rule for them and a run of authoritarian rules for us Angry doesnt even cover it\n",
      "\u001b[32mCosine Similarity: 0.42\n",
      "\u001b[0m\n",
      "Absolute disgrace They didnt break the rules to see dying relatives They broke the rules to get drunk in Downing Street 99Percent I think this is a nonstory Asked about the latest round of fines for Covid rulebreaking in Downing Street Co\n",
      "\u001b[32mCosine Similarity: 0.42\n",
      "\u001b[0m\n",
      "10 Downing Street has become a Den of Iniquity and Inequity Boris Johnson and his cabinet have brought our nation to its knees with their cheating lying evil doing and injustices\n",
      "\u001b[32mCosine Similarity: 0.41\n",
      "\u001b[0m\n",
      "Boris Johnson is so ludicrously self serving how blatant is this How do the MPs that support him keep going along with it This is an appalling abhorrent and increasingly autocratic leadership\n",
      "\u001b[32mCosine Similarity: 0.41\n",
      "\u001b[0m\n",
      "At least is honest about one thing sadly it is about the incredulous behaviour that took place at Downing Street Asked about the conduct described in the Sue Gray report Boris Johnson told MPs Id do it again\n",
      "\u001b[32mCosine Similarity: 0.41\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "most_similar_to_random_tweet(df, embedding, annoy_index, 0.4, 2, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Similarity Graph\n",
    "\n",
    "Construct a graph where vertices are tweets and edges represent index similarity (for closely related tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(word_vectors, index, min_sim_score=0.5):\n",
    "    \n",
    "    # Edge iterator for the graph\n",
    "    def gen_edges():\n",
    "\n",
    "        for src_id in tqdm(range(len(word_vectors.index_to_key))):       \n",
    "            vector = word_vectors.get_vector(src_id)\n",
    "            dists = word_vectors.most_similar([vector], indexer=index)\n",
    "\n",
    "            for tgt_id, score in dists:\n",
    "                if int(tgt_id) > src_id and score > min_sim_score:\n",
    "                    yield (src_id, int(tgt_id), score)\n",
    "                    \n",
    "    \n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add edges.\n",
    "    for i,j,w in gen_edges():\n",
    "        G.add_edge(i, j, weight=w)\n",
    "\n",
    "    # Now remove self-edges. (Every document is most similar to itself, but this is not interesting.)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))    \n",
    "    \n",
    "    for idx, doc in tqdm(enumerate(df.preprocessed_text)):\n",
    "        G.add_node(idx, tokens=doc)\n",
    "\n",
    "    print(f\"Vertices: [{G.number_of_nodes()}], Edges: [{G.number_of_edges()}]\")\n",
    "\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    print(f\"After dropping singletons: Vertices: [{G.number_of_nodes()}], Edges: [{G.number_of_edges()}]\")    \n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [01:01<00:00, 1627.12it/s]\n",
      "100000it [00:00, 862326.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertices: [100000], Edges: [211285]\n",
      "After dropping singletons: Vertices: [71061], Edges: [211285]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if REBUILD:\n",
    "        raise FileNotFoundError()\n",
    "    else:\n",
    "        print(\"Loading Graph\")\n",
    "        G = nx.readwrite.gpickle.read_gpickle(f\"./graphs/graph.gpickle\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Generating Graph\")\n",
    "    G = generate_graph(embedding, annoy_index, min_sim_score=MIN_SIM_SCORE)\n",
    "    nx.readwrite.gpickle.write_gpickle(G, f\"./graphs/graph.gpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Clusters of Similar Tweets\n",
    "\n",
    "Using one of my \"go-to\" community detection methods, Label Propagation (https://arxiv.org/abs/0803.0476).  Selected because it is fast and there is a simple-to-implement version for large graphs in a distributed setting, which would help us with productionisation.\n",
    "\n",
    "\"Modularity\" is a measure of how good a particular partitioning of a graph into disjoint subgraphs is.  It's useful to gauge whether the community-detection step is working well.  See https://en.wikipedia.org/wiki/Modularity_(networks) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metric = namedtuple('Metric', 'C Q r')\n",
    "\n",
    "def cluster_label_propogation(G, verbose=False):\n",
    "\n",
    "    clusters = label_propagation(G)\n",
    "    Q = clusters.newman_girvan_modularity().score\n",
    "    C = len(clusters.communities)\n",
    "\n",
    "    # Singletons are not considered to be clusters and will be removed in subsequent steps.\n",
    "    n_tweets = G.number_of_nodes()\n",
    "    n_clusters = len([c for c in clusters.communities if len(c) > 1])\n",
    "    largest_cluster = max([len(c) for c in clusters.communities])\n",
    "    n_assigned = sum([len(c) for c in clusters.communities if len(c) > 1])\n",
    "    n_unassigned = n_tweets - n_assigned\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\"\"\n",
    "            There are [{n_tweets}] tweets.\n",
    "            [{n_clusters}] clusters were identified.\n",
    "            The largest has [{largest_cluster}] members.\n",
    "            [{n_assigned}] tweets assigned to a cluster.\n",
    "            [{n_unassigned}] tweets were not assigned to a cluster.\n",
    "            Modularity is {np.round(Q, 2)}.\n",
    "        \"\"\")\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            There are [71061] tweets.\n",
      "            [10423] clusters were identified.\n",
      "            The largest has [474] members.\n",
      "            [71061] tweets assigned to a cluster.\n",
      "            [0] tweets were not assigned to a cluster.\n",
      "            Modularity is 0.8.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "clusters = cluster_label_propogation(G, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this notebook we're going to drop small topics.  In a production context we need to consider what we do:\n",
    "- If we drop small topics then we might lose important information: e.g. if a topic grows over time then we'd miss out on it's genesis.\n",
    "- If we generate all topics then we might create too much data.  After all, the large majority of small topics will never be used in a report or even viewed by a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x241fb86a280>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clusters = [c for c in clusters.communities if len(c) >= MIN_CLUSTER_SIZE]\n",
    "cluster_sizes = pd.Series({k: len(v) for k,v in enumerate(clusters)})\n",
    "sns.displot(cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the clusters to the graph\n",
    "for cluster_id, members in enumerate(clusters):\n",
    "    for m in members:\n",
    "        G.add_node(m, cluster_id=cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory test: print samples from clusters\n",
    "\n",
    "And now the moment we've been waiting for: how good are these things, exactly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_cluster(df, clusters, cluster_sizes, sample_size=10, cluster_id=None):\n",
    "    \n",
    "    if not cluster_id:\n",
    "        cluster_id = np.random.choice(cluster_sizes.index)\n",
    "\n",
    "    print(colored(f\"Cluster [{cluster_id}] has [{cluster_sizes[cluster_id]}] members.\\n\", \"red\", attrs=['bold']))\n",
    "\n",
    "    cluster = clusters[cluster_id]\n",
    "    sample = np.random.choice(cluster, min(sample_size, len(cluster)))    \n",
    "\n",
    "    for doc_id in sample:\n",
    "        print(df.tweet_text.values[doc_id].replace('\\n', ''), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCluster [257] has [34] members.\n",
      "\u001b[0m\n",
      "Bring back rationing? Will the mindless Facebook nostalgia never end?  \"🏋️When meat and all other food rationing ended in Britain in June 1954, it left the nation fitter than it had been… \" \n",
      "\n",
      "Apocalyptic food prices, ©️Governor of The Bank of England, are the price of Tory Britain. \n",
      "\n",
      "@castlvillageman @DbAshby MEGA BRITISH FOOD SHORTAGES.  BIG TIME. \n",
      "\n",
      "£0.30/day to eat fresh food cooked from scratch if people are taught to \"cook properly\"? That new cookbook will sure be a best seller... \n",
      "\n",
      "Cheap food in Britain is about to become a thing of the past - Bloomberg \n",
      "\n",
      "A whole lot of pain is coming to consumers in Brexit Britain…Cheap Food in Britain Is About to Become a Thing of the Past - Bloomberg \n",
      "\n",
      "Cheap food in Britain is about to become a thing of the past - Bloomberg - You can have a racist, corrupt &amp; incompetent govt or you can have cheap food &amp; economic prosperity but you CAN’T have both! In #GE2019 Brits chose the latter, HELLO Food poverty! \n",
      "\n",
      "£0.30/day to eat fresh food cooked from scratch if people are taught to \"cook properly\"? That new cookbook will sure be a best seller... \n",
      "\n",
      "@castlvillageman @DbAshby MEGA BRITISH FOOD SHORTAGES.  BIG TIME. \n",
      "\n",
      "Bank of England Governor Warns Of ‘Apocalyptic’ Food Price Rises Due To Ukraine WarProbably not the best time to be contemplating a trade war with the EU, really.Story by @SophiaSleigh ⬇️ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_cluster(df, clusters, cluster_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Note on Evaluation\n",
    "\n",
    "There are really only three methods for evaluating topic models:\n",
    "\n",
    "1. Comparison with a \"ground truth\".  Where labelled datasets are available (for example the benchmark \"20newsgroups\" dataset frequently used to evaluate topic models) it is straightforward to measure correspondence.  This approach was not available for this model.\n",
    "2. Topic _coherence_ or _pereplexity_.  These are standard measures that describe the internal consistency of the found topics.  Both require that the approach taken yields a probabilistic model (and so are frequently used to evaluate modern Dirichlet-process based models) but will not be useful here.  Other methods (for example based on _information gain_) require that topics be represented as distributions of words.  Again, something not explicitly done here.\n",
    "3. Human judgement.  This is probably the most important method when building a topic model for a real-world use case.  How useful do humans find the results?  \n",
    "\n",
    "The algorithm in this workbook was developed and tested in conjunction with Public Affairs experts who were using the results to calculate the trends and flows of particular stories online.  Those individuals' judgement were the main evaluation criteria we used.\n",
    "\n",
    "Clearly I would like to tune the various hyperparameters in this algorithm to maximise some kind of measure of topic quality.  I'll post an update to the workbook when I think of one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring new values\n",
    "\n",
    "Finally, a quick example to show how new-arriving data can be scored.  The beauty of using a BERT model is that the wordpiece tokenisation step enables us to easily incorporate new documents containing out-of-vocabulary words (something that has traditionally stymied probabilistic and matrix-based topic-models that rely on word-tokenisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[1m\u001b[31mNEW DOCUMENT\u001b[0m\n",
      "\u001b[1mConservative MP Lee Anderson says there isnt a massive need for food banks in the UK\u001b[0m \n",
      "\n",
      "\u001b[1m\u001b[32mAssigned to Cluster [526] which has [22] members.\n",
      "\u001b[0m\n",
      "\u001b[4m\u001b[1m\u001b[31mSIMILAR DOCUMENTS\u001b[0m\n",
      "Clueless #tosser: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "Come on, Ashfield, vote this idiot, #LeeAnderson out next time.: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "Come on, Ashfield, vote this idiot, #LeeAnderson out next time.: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "Anderson can fuck his own fist for the vicious stupidity of this comment.: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "An MP thinking foodbanks are street food stalls: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "Conservative MP Lee Anderson is an a*se.: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "Complete Tanker! 😡🤮@AdamBienkov @leeandersonmp @LeeAndersonMP1 #BorisMustGo: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "Dear God 😞: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "Conservative MP Lee Anderson is an a*se.: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "Complete Tanker! 😡🤮@AdamBienkov @leeandersonmp @LeeAndersonMP1 #BorisMustGo: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "new = \"Conservative MP Lee Anderson says there isnt a massive need for food banks in the UK\"\n",
    "\n",
    "vec = model.encode(\n",
    "    new, \n",
    "    convert_to_numpy=True, \n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "dists = embedding.most_similar([vec], indexer=annoy_index)\n",
    "dists = [(doc_id, score) for doc_id, score in dists if score >= MIN_SIM_SCORE and score < 1.0]\n",
    "C = Counter([G.nodes[id]['cluster_id'] for id, _ in dists])\n",
    "cluster_id = C.most_common()[0][0]\n",
    "\n",
    "print(colored(\"NEW DOCUMENT\", 'red', attrs=['bold', 'underline']))\n",
    "print(colored(new, attrs=['bold']), '\\n')\n",
    "print(colored(f\"Assigned to Cluster [{cluster_id}] which has [{cluster_sizes[cluster_id]}] members.\\n\", 'green', attrs=['bold']))\n",
    "print(colored(\"SIMILAR DOCUMENTS\", 'red', attrs=['bold', 'underline']))\n",
    "\n",
    "cluster = clusters[cluster_id]\n",
    "sample = np.random.choice(cluster, min(10, len(cluster)))    \n",
    "\n",
    "for doc_id in sample:\n",
    "    print(df.tweet_text.values[doc_id].replace('\\n', ''), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
