{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction & Motivation\n",
    "\n",
    "Topic models take a corpus of human-readable text documents and cluster them.  The idea is that documents are assigned to the same cluster when they are \"about\" the same topic.  The field has advanced a long way since the introduction of _Latent Dirichlet Allocation_ back in 2003.  These days deep learning topic models are in vogue.  In particular, BERT-based topic models are appealing because they can naturally handle out-of-vocabulary tokens and generate powerful text embeddings.\n",
    "\n",
    "This notebook is a stripped-down version of something I created for finding granular topics in short-form social media texts.  I had been experimenting with many published models - including Bayesian ones, non-negative matrix factrisation methods and deep-learning based ones but was not getting the results I wanted.  My particular use case required low-level, granular topics - essentially I wanted to define a topic as a particular variation of a claim, an event or a specifc belief.  Everything I tried was either returning broader topics (\"COVID 19\", \"war in ukraine\", etc) - or it was totally failing to get to grips with the truncated nature of short-form posts (like tweets).\n",
    "\n",
    "I took inspiration from Dimo Angelov's **top2vec** paper (https://arxiv.org/abs/2008.09470).  The author bolted together an embedding layer - which generated 768-dimensional, dense vectors - with UMAP to reduce said dimensionality and finally HDBScan to yield a partial clustering of the documents.  The centroid of the found clusters could then be reprojected into the embedding space, yielding a joint embedding of documents and topics.\n",
    "\n",
    "Lovely though the process is I couldn't achieve good results on my short-form data at the level of granularity I wanted.  So I have adapted Dimo's approach, and it works really nicely.  What follows is a first step towards a model that clusters tweets at the level of a particular claim, statistic or event.  (This is not about identifying or evaluating claims and opinions _per se_.)\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "1. Properly prepare the tweets, removing URLS and hashtags (which tend to be unhelpful when they appear in groups).\n",
    "2. Fine-tune a BERT model using SimCSE - a form of Multiple Negative-Ranking Loss.  This yields an embedding tuned to the specific domains of the data I am working with.\n",
    "3. Use the fine-tuned model to embed the tweets.\n",
    "4. Build an (approximate) neareset-neighbours index over the embedded tweets.  This enables us to efficiently find similar tweets without having to make $\\mathcal{O}\\left(N^2\\right)$ comparisons.\n",
    "5. Build a graph where each tweet is a node and each nearest neighbour (above a similarity threshold) is an edge.\n",
    "6. Run a simple community detection algorithm over the graph.\n",
    "\n",
    "Voila: topics.\n",
    "\n",
    "### A Word on Data\n",
    "\n",
    "This notebook uses a set of tweets, collected in 2022 with keywords that relate to the cost of living crisis and the then Prime Minister's alleged breaches of the UK's COVID-19 lockdown rules.  Twitter's terms of use do not allow for their data to be shared, but this notebook will work on any set of tweets you may have to hand (I've tested it against lots of them!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Will\\Anaconda3\\envs\\deepseer3\\lib\\site-packages\\huggingface_hub\\snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'wurlitzer', 'graph_tool', 'leidenalg', 'karateclub'}\n",
      "Note: to be able to use all overlapping methods, you need to install some additional packages:  {'ASLPAw', 'karateclub'}\n",
      "Note: to be able to use all bipartite methods, you need to install some additional packages:  {'wurlitzer', 'leidenalg'}\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "from textacy import preprocessing\n",
    "from functools import partial\n",
    "from sentence_transformers import SentenceTransformer, InputExample, models, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.similarities.annoy import AnnoyIndexer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from networkx import NetworkXError\n",
    "from cdlib.algorithms import label_propagation\n",
    "from collections import namedtuple, Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20,10)})\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42                 # For repeatable experiments\n",
    "REBUILD = True                   # Set to TRUE to force a rebuild of the core transformer, embeddings, Annoy index and sim-graph\n",
    "\n",
    "# Pre-processing \n",
    "MAX_HASHTAGS_BEFORE_REMOVAL = 0  # If there are more than this many hashtags in a tweet, remove them all.\n",
    "\n",
    "# Fine-tuning\n",
    "N_TRAINING_EPOCHS = 2            # Number of epochs of fine-tuning for the transformer.\n",
    "MAX_SEQ_LEN = 140                # Large enough for the vast majority of tweets.\n",
    "BATCH_SIZE = 32                  # Larger = better for SimCSE\n",
    "\n",
    "# Indexing\n",
    "NUM_ANNOY_TREES = 1              # How many NN trees to build.  More trees = more accurate recall.\n",
    "\n",
    "# Topic Graph\n",
    "MIN_SIM_SCORE = 0.45             # Minimum similarity between embedding vectors to yield an edge in the topic graph.\n",
    "MIN_CLUSTER_SIZE = 10            # Discard topic clusters with fewer than this number of members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Minimally clean the data, making it suitable for BERT finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains [100000] rows.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"./data/data.csv\").dropna()\n",
    "print(f\"Dataset contains [{df.shape[0]}] rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip URLs\n",
    "\n",
    "First find all of the t.co URLs and place them into a new column.  Remove them from the tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS_RE = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "\n",
    "df['urls'] = df.tweet_text.apply(URLS_RE.findall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the URLs from the tweet text\n",
    "\n",
    "def remove_urls(row):\n",
    "    tweet_text = row.tweet_text\n",
    "    for u in row.urls:\n",
    "         tweet_text = tweet_text.replace(u, '')\n",
    "    return tweet_text.strip()\n",
    "\n",
    "df.tweet_text = df.apply(remove_urls, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Text Preprocessing\n",
    "\n",
    "The only point here of note is the handling of hashtags.  Tweets often contain a large number of hashtags appended to the main body of the text.  I've found these actually _detract_ from the ability of topic models to do their work properly.  So in the pre-processing code below, if there are more than `MAX_HASHTAGS_BEFORE_REMOVAL` hashtags then _all_ of them will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_user_handles = partial(preprocessing.replace.user_handles, repl='')\n",
    "replace_urls = partial(preprocessing.replace.urls, repl='')\n",
    "replace_hashtags = partial(preprocessing.replace.hashtags, repl='')\n",
    "remove_ampersand = lambda text: text.replace('&amp', '')\n",
    "remove_newline = lambda text: text.replace('\\n', ' ')\n",
    "replace_emojis = partial(preprocessing.replace.emojis, repl='')\n",
    "delete_chars = lambda text: text.replace(\"'\", \"\").replace(\"-\", \"\")\n",
    "\n",
    "preproc = preprocessing.make_pipeline(\n",
    "    preprocessing.normalize.bullet_points,\n",
    "    preprocessing.normalize.quotation_marks,\n",
    "    preprocessing.normalize.whitespace,\n",
    "    preprocessing.normalize.unicode,\n",
    "    preprocessing.remove.accents,\n",
    "    replace_urls,\n",
    "    remove_ampersand,\n",
    "    delete_chars,    \n",
    "    replace_user_handles,\n",
    "    preprocessing.remove.punctuation,    \n",
    "    replace_emojis,\n",
    "    remove_newline,\n",
    "    preprocessing.remove.brackets,\n",
    "    preprocessing.normalize.whitespace\n",
    ")\n",
    "\n",
    "preproc_no_hashtags = preprocessing.make_pipeline(\n",
    "    preprocessing.normalize.bullet_points,\n",
    "    preprocessing.normalize.quotation_marks,\n",
    "    preprocessing.normalize.whitespace,\n",
    "    preprocessing.normalize.unicode,\n",
    "    preprocessing.remove.accents,\n",
    "    replace_urls,\n",
    "    remove_ampersand,    \n",
    "    # Replace hashtags with the empty string before we handle punctuation\n",
    "    replace_hashtags,\n",
    "    replace_user_handles,\n",
    "    delete_chars,\n",
    "    preprocessing.remove.punctuation,\n",
    "    replace_emojis,\n",
    "    remove_newline,\n",
    "    preprocessing.remove.brackets,\n",
    "    preprocessing.normalize.whitespace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, hashtag_threshold=MAX_HASHTAGS_BEFORE_REMOVAL):\n",
    "    if text.count(\"#\") > hashtag_threshold:\n",
    "        return preproc_no_hashtags(text)\n",
    "    else:\n",
    "        return preproc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = [preprocess(t) for t in df.tweet_text.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the SimCSE transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimCSE model (https://arxiv.org/abs/2104.08821) works as follows:\n",
    "\n",
    "1. Start with a pretrained BERT transformer model to obtain fixed-dimensional word embeddings.  We use DistilRoBERTa (https://huggingface.co/docs/transformers/model_doc/distilbert) as the base. This is based on the RoBERTa model which only trains against the Masked Language Model (MLM) objective.  Dropout is set to 0.1 as standard.\n",
    "2. Stack a mean-pooling layer on top.  (The `all-distilroberta-v1` model was trained using mean-pooling, not the CLS token, to generate document embeddings.)\n",
    "3. Training examples consist of pairs of the same document (dropout masks are be applied separately, and randomly, to each item in a pair).\n",
    "4. Use the Multiple Negatives Ranking (MNR) Loss.  For example $x_i$ in a batch of $K$ input pairs the loss function evaluates:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{l}_i &= - \\text{log } \\mathbf{P}\\left(x_i | x_i\\right) \\\\\n",
    "& \\approx -\\text{log } \\left( \\frac{e^{\\text{sim}\\left(z_i^{\\left(1\\right)}, z_i^{\\left(2\\right)}\\right)}}{\\sum_{k=1}^{K}e^{\\text{sim}\\left(z_i^{\\left(1\\right)}, z_k^{\\left(2\\right)}\\right)}} \\right) \\\\\n",
    "&\\approx -\\text{sim}\\left(z_i^{\\left(1\\right)}, z_i^{\\left(2\\right)}\\right) + \\text{log } \\left(\\sum_{k=1}^{K}e^{\\text{sim}\\left(z_i^{\\left(1\\right)}, z_k^{\\left(2\\right)}\\right)} \\right)\n",
    "\\end{align}\n",
    "Where $\\text{sim}$ is the cosine similarity function, $z_i$ is the mean-embedding vector for example $x_i$ and the superscripts $z_i^{\\left(1\\right)}$ and $z_i^{\\left(2\\right)}$ denote the dropout masks applied to the first and second elements of the input pair, respectively.  (MNR loss is therefore effectively just a form of cross-entropy loss.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def finetune_simcse():\n",
    "\n",
    "    # Cased version of DistilRoBERTa.\n",
    "    model_name = 'sentence-transformers/all-distilroberta-v1'\n",
    "\n",
    "    word_embedding_model = models.Transformer(model_name, max_seq_length=MAX_SEQ_LEN)\n",
    "\n",
    "    pooling_model = models.Pooling(\n",
    "        word_embedding_dimension=word_embedding_model.get_word_embedding_dimension(),\n",
    "        pooling_mode='mean'\n",
    "    )\n",
    "\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    # SimCSE uses masked pairs of inputs.\n",
    "    train_data = [InputExample(texts=[doc, doc]) for doc in df.preprocessed_text.to_numpy()]\n",
    "\n",
    "    # Batch the data.  Larger batches make the MNR loss more accurate.\n",
    "    # Shuffling super-important, otherwise the MNR loss will not work \n",
    "    # (because we risk near identical tweets appearing next to each other in the dataset).\n",
    "    train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "    \n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=N_TRAINING_EPOCHS,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    model.save(f\"./transformers/model\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning using SimCSE task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53539ec83da40599a94cce7d663e8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35984167c974f2aac29dab079809769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11548c7700c427baf920877ca0a0a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load or finetune a SimCSE model\n",
    "\n",
    "try:\n",
    "    if REBUILD:\n",
    "        raise ValueError()\n",
    "    else:\n",
    "        print(\"Loading transformer model\")\n",
    "        model = SentenceTransformer(f\"./transformers/model\")\n",
    "except ValueError:\n",
    "    print(\"Fine-tuning using SimCSE task\")    \n",
    "    model = finetune_simcse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate the embeddings\n",
    "\n",
    "Run the model in execution mode (no dropouts) to generate embedded vectors.  The vectors are wrapped in Gensim's `KeyedVectors` class and form inputs to the indexing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d5dd2aec604d1c9fde798e197a8350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    if REBUILD:\n",
    "        raise FileNotFoundError()\n",
    "    else:\n",
    "        print(\"Loading Embedding\")        \n",
    "        embedding = KeyedVectors.load(f\"./embeddings/embedding__x768.kv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Generating Embedding\")\n",
    "    embedding = KeyedVectors(model[1].word_embedding_dimension)\n",
    "    embedding.add_vectors(\n",
    "        keys=range(n_docs),\n",
    "        weights=model.encode(\n",
    "            df.preprocessed_text.to_numpy(), \n",
    "            show_progress_bar=True, \n",
    "            convert_to_numpy=True, \n",
    "            device='cuda'\n",
    "        )\n",
    "    )\n",
    "    embedding.save(f\"./embeddings/embedding__x768.kv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search\n",
    "\n",
    "We proceed as follows:\n",
    "\n",
    "Construct an efficient index for performing similarity search over our embedding vectors.  I have chosen Approximate Nearest Neighbours (A-NN), implemented by the `annoy` package (https://github.com/spotify/annoy), which comes packaged with `gensim`.  It has the following properties:\n",
    "  1. It is stochastic (i.e. it doesn't guarantee the same - or a complete - similarity set each time you run it).  However, given subsequent steps this has minimal impact on the clustering results.\n",
    "  2. `annoy`, specifically, is fast - making it well suited to prototyping.\n",
    "  3. Nearest Neighbours, in general, is simple.  For implementing at scale we might want to compare A-NN with Locality Sensitive Hashing.\n",
    "  4. The default distance metric used by `annoy` is Cosine similarity, which is the same metric as was used during SimCSE training.  It'll also perform better than, say the Euclidean distance on a length 768 dense vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Annoy Index\n"
     ]
    }
   ],
   "source": [
    "annoy_index = AnnoyIndexer()\n",
    "\n",
    "try:\n",
    "    if REBUILD:\n",
    "        raise OSError()\n",
    "    else:\n",
    "        print(\"Loading Annoy Index\")\n",
    "        annoy_index.load(f\"./annoy_indexes/index.annoy\")\n",
    "except OSError:\n",
    "    print(\"Creating Annoy Index\")\n",
    "    annoy_index = AnnoyIndexer(embedding, num_trees=NUM_ANNOY_TREES)\n",
    "    annoy_index.save(f\"./annoy_indexes/index.annoy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory test: use the index to find examples of similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def most_similar_to_random_tweet(df, embedding, annoy_index, min_sim_score, min_matches=1, n_trials=100):\n",
    "    for i in range(n_trials):\n",
    "        random_doc_id = np.random.choice(df.shape[0])\n",
    "        vector = embedding.get_vector(random_doc_id)\n",
    "        dists = embedding.most_similar([vector], indexer=annoy_index)\n",
    "        dists = [(doc_id, score) for doc_id, score in dists if score >= min_sim_score and score < 1.0]\n",
    "        \n",
    "        if len(dists) >= min_matches:\n",
    "            print(colored(\"TEST TWEET\", \"red\", attrs=['bold', 'underline']))\n",
    "            print(colored(df.preprocessed_text.values[random_doc_id], attrs=['bold']))\n",
    "            print(colored(f\"DOC_ID: {random_doc_id}\", \"blue\"))\n",
    "            print(colored(\"\\nMATCHES\", \"red\", attrs=['bold', 'underline']))\n",
    "            \n",
    "            for doc_id, score in dists:\n",
    "                doc_id = int(doc_id)\n",
    "                print(df.preprocessed_text.values[doc_id])\n",
    "                print(colored(f\"Cosine Similarity: {np.round(score, 2)}\", \"green\"))\n",
    "                print(colored(f\"DOC_ID: {doc_id}\\n\", \"blue\"))\n",
    "            return None\n",
    "        \n",
    "    print(f\"{n_trials} tweets sampled but none had >= {min_matches} matches with score >= {min_sim_score}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[1m\u001b[31mTEST TWEET\u001b[0m\n",
      "\u001b[1mBritain wants to revive imperial measurements to mark Queens jubilee market traders welcomed the moved but said the postBrexit plans wont shore up support for Boris Johnson\u001b[0m\n",
      "\u001b[34mDOC_ID: 67984\u001b[0m\n",
      "\u001b[4m\u001b[1m\u001b[31m\n",
      "MATCHES\u001b[0m\n",
      "EXCL Boris Johnson plans to announce imperial measurements will be revived to mark Queens Platinum Jubilee and boost flagging support among Brexit voters\n",
      "\u001b[32mCosine Similarity: 0.58\u001b[0m\n",
      "\u001b[34mDOC_ID: 89106\n",
      "\u001b[0m\n",
      "Boris Johnson to announce the revival of imperial measurements to mark the Queens Platinum Jubilee\n",
      "\u001b[32mCosine Similarity: 0.57\u001b[0m\n",
      "\u001b[34mDOC_ID: 57964\n",
      "\u001b[0m\n",
      "Boris Johnson is planning to announce that imperial measurements will be revived to mark the Queens Platinum Jubilee reports for\n",
      "\u001b[32mCosine Similarity: 0.56\u001b[0m\n",
      "\u001b[34mDOC_ID: 49130\n",
      "\u001b[0m\n",
      "Boris Johnson will announce return of imperial measurements to mark Queens Platinum Jubilee\n",
      "\u001b[32mCosine Similarity: 0.55\u001b[0m\n",
      "\u001b[34mDOC_ID: 60267\n",
      "\u001b[0m\n",
      "Boris Johnson to reportedly bring back imperial measurements to mark platinum jubilee Move is an apparent attempt to win support from Brexit voters in seats Tories fear losing Boris Johnson will reportedly announce the return of imperial VIA\n",
      "\u001b[32mCosine Similarity: 0.55\u001b[0m\n",
      "\u001b[34mDOC_ID: 58358\n",
      "\u001b[0m\n",
      "Boris Johnson set to bring back imperial measurements to mark platinum jubilee\n",
      "\u001b[32mCosine Similarity: 0.54\u001b[0m\n",
      "\u001b[34mDOC_ID: 54853\n",
      "\u001b[0m\n",
      "Boris Johnson set to bring back imperial measurements to mark platinum jubilee\n",
      "\u001b[32mCosine Similarity: 0.54\u001b[0m\n",
      "\u001b[34mDOC_ID: 54854\n",
      "\u001b[0m\n",
      "Boris Johnson will reportedly announce the return of imperial measurements to mark the Queens platinum jubilee in an apparent attempt to garner support among Brexiter voters\n",
      "\u001b[32mCosine Similarity: 0.53\u001b[0m\n",
      "\u001b[34mDOC_ID: 60501\n",
      "\u001b[0m\n",
      "Boris Johnson to reportedly bring back imperial measurements to mark platinum jubilee\n",
      "\u001b[32mCosine Similarity: 0.52\u001b[0m\n",
      "\u001b[34mDOC_ID: 58332\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "most_similar_to_random_tweet(df, embedding, annoy_index, 0.5, 5, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Similarity Graph\n",
    "\n",
    "Construct a graph where vertices are tweets and edges represent index similarity (for closely related tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(word_vectors, index, min_sim_score=0.5):\n",
    "    \n",
    "    # Edge iterator for the graph\n",
    "    def gen_edges():\n",
    "\n",
    "        for src_id in tqdm(range(len(word_vectors.index_to_key))):       \n",
    "            vector = word_vectors.get_vector(src_id)\n",
    "            dists = word_vectors.most_similar([vector], indexer=index)\n",
    "\n",
    "            for tgt_id, score in dists:\n",
    "                if int(tgt_id) > src_id and score > min_sim_score:\n",
    "                    yield (src_id, int(tgt_id), score)\n",
    "                    \n",
    "    \n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add edges.\n",
    "    for i,j,w in gen_edges():\n",
    "        G.add_edge(i, j, weight=w)\n",
    "\n",
    "    # Now remove self-edges. (Every document is most similar to itself, but this is not interesting.)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))    \n",
    "    \n",
    "    for idx, doc in tqdm(enumerate(df.preprocessed_text)):\n",
    "        G.add_node(idx, tokens=doc)\n",
    "\n",
    "    print(f\"Vertices: [{G.number_of_nodes()}], Edges: [{G.number_of_edges()}]\")\n",
    "\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    print(f\"After dropping singletons: Vertices: [{G.number_of_nodes()}], Edges: [{G.number_of_edges()}]\")    \n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [01:06<00:00, 1510.64it/s]\n",
      "100000it [00:00, 833345.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertices: [100000], Edges: [121093]\n",
      "After dropping singletons: Vertices: [53188], Edges: [121093]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if REBUILD:\n",
    "        raise FileNotFoundError()\n",
    "    else:\n",
    "        print(\"Loading Graph\")\n",
    "        G = nx.readwrite.gpickle.read_gpickle(f\"./graphs/graph.gpickle\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Generating Graph\")\n",
    "    G = generate_graph(embedding, annoy_index, min_sim_score=MIN_SIM_SCORE)\n",
    "    nx.readwrite.gpickle.write_gpickle(G, f\"./graphs/graph.gpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Clusters of Similar Tweets\n",
    "\n",
    "Using one of my \"go-to\" community detection methods, Label Propagation (https://arxiv.org/abs/0803.0476).  Selected because it is fast and there is a simple-to-implement version for large graphs in a distributed setting, which would help us with productionisation.\n",
    "\n",
    "\"Modularity\" is a measure of how good a particular partitioning of a graph into disjoint subgraphs is.  It's useful to gauge whether the community-detection step is working well.  See https://en.wikipedia.org/wiki/Modularity_(networks) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metric = namedtuple('Metric', 'C Q r')\n",
    "\n",
    "def cluster_label_propogation(G, verbose=False):\n",
    "\n",
    "    clusters = label_propagation(G)\n",
    "    Q = clusters.newman_girvan_modularity().score\n",
    "    C = len(clusters.communities)\n",
    "\n",
    "    # Singletons are not considered to be clusters and will be removed in subsequent steps.\n",
    "    n_tweets = G.number_of_nodes()\n",
    "    n_clusters = len([c for c in clusters.communities if len(c) > 1])\n",
    "    largest_cluster = max([len(c) for c in clusters.communities])\n",
    "    n_assigned = sum([len(c) for c in clusters.communities if len(c) > 1])\n",
    "    n_unassigned = n_tweets - n_assigned\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\"\"\n",
    "            There are [{n_tweets}] tweets.\n",
    "            [{n_clusters}] clusters were identified.\n",
    "            The largest has [{largest_cluster}] members.\n",
    "            [{n_assigned}] tweets assigned to a cluster.\n",
    "            [{n_unassigned}] tweets were not assigned to a cluster.\n",
    "            Modularity is {np.round(Q, 2)}.\n",
    "        \"\"\")\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            There are [53188] tweets.\n",
      "            [11526] clusters were identified.\n",
      "            The largest has [202] members.\n",
      "            [53188] tweets assigned to a cluster.\n",
      "            [0] tweets were not assigned to a cluster.\n",
      "            Modularity is 0.87.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "clusters = cluster_label_propogation(G, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this notebook we're going to drop small topics.  In a production context we need to consider what we do:\n",
    "- If we drop small topics then we might lose important information: e.g. if a topic grows over time then we'd miss out on it's genesis.\n",
    "- If we generate all topics then we might create too much data.  After all, the large majority of small topics will never be used in a report or even viewed by a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x294e4ebabe0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clusters = [c for c in clusters.communities if len(c) >= MIN_CLUSTER_SIZE]\n",
    "cluster_sizes = pd.Series({k: len(v) for k,v in enumerate(clusters)})\n",
    "sns.displot(cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the clusters to the graph\n",
    "for cluster_id, members in enumerate(clusters):\n",
    "    for m in members:\n",
    "        G.add_node(m, cluster_id=cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory test: print samples from clusters\n",
    "\n",
    "And now the moment we've been waiting for: how good are these things, exactly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_cluster(df, clusters, cluster_sizes, sample_size=10, cluster_id=None):\n",
    "    \n",
    "    if not cluster_id:\n",
    "        cluster_id = np.random.choice(cluster_sizes.index)\n",
    "\n",
    "    print(colored(f\"Cluster [{cluster_id}] has [{cluster_sizes[cluster_id]}] members.\\n\", \"red\", attrs=['bold']))\n",
    "\n",
    "    cluster = clusters[cluster_id]\n",
    "    sample = np.random.choice(cluster, min(sample_size, len(cluster)))    \n",
    "\n",
    "    for doc_id in sample:\n",
    "        print(df.tweet_text.values[doc_id].replace('\\n', ''), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCluster [828] has [11] members.\n",
      "\u001b[0m\n",
      "CPI inflation up to 9% in April, more than double latest regular pay growth of 4.2%. So unless you’re a banker with a big bonus, real earnings are falling fast. This is despite a tight labour market with more vacancies than unemployed people. \n",
      "\n",
      "CPI inflation up to 9% in April, more than double latest regular pay growth of 4.2%. So unless you’re a banker with a big bonus, real earnings are falling fast. This is despite a tight labour market with more vacancies than unemployed people. \n",
      "\n",
      "@AffairNational @PMOIndia @narendramodi @AmitShah @myogiadityanath @Mansi94893004 @hinaparvezbutt @MaryamNSharif @OfficialDGISPR @Bajwas24 @POTUS @VP @HillaryClinton @BorisJohnson @trussliz @RoyalFamily @antonioguterres @IMFNews @WorldBank @WSJecon @WSJ @cnnbrk @CMShehbaz @HamzaSS @Newsweek inflation rose to an average of 9.8% y-o-y in H1 FY22 from 8.6% in H1 FY21, driven by surging global commodity prices and weaker exchange rate Similarly, core inflation has been increasing since September 2021.Pakistan has been unwinding its expansionary monetary stance since2021 \n",
      "\n",
      "Consumer spending, which accounts for more than two-thirds of US economic activity, increased 0.9% last month, and although inflation continued to increase in April, it was less than in recent months \n",
      "\n",
      "Annual producer inflation in the Euro Area increased to a new record high of 37.2% in April from an upwardly revised 36.9% in March, showing that inflationary pressures in Europe remain elevated and have not peaked yet. \n",
      "\n",
      "At the same time, US retail sales remain quite elevated, at least in nominal (non-inflation-adjusted) terms. Sales rose +0.9% m/m in April, up +8.2% from a year ago. \n",
      "\n",
      "@AffairNational @PMOIndia @narendramodi @AmitShah @myogiadityanath @Mansi94893004 @hinaparvezbutt @MaryamNSharif @OfficialDGISPR @Bajwas24 @POTUS @VP @HillaryClinton @BorisJohnson @trussliz @RoyalFamily @antonioguterres @IMFNews @WorldBank @WSJecon @WSJ @cnnbrk @CMShehbaz @HamzaSS @Newsweek inflation rose to an average of 9.8% y-o-y in H1 FY22 from 8.6% in H1 FY21, driven by surging global commodity prices and weaker exchange rate Similarly, core inflation has been increasing since September 2021.Pakistan has been unwinding its expansionary monetary stance since2021 \n",
      "\n",
      "Consumer spending, which accounts for more than two-thirds of US economic activity, increased 0.9% last month, and although inflation continued to increase in April, it was less than in recent months \n",
      "\n",
      "At a time when inflation is hurting income growth, a 0.9% month-on-month rise in retail sales for April is a promising sign.Sales are up 29% from January 2020, and the core figures, which exclude food and energy, were much better than expected. \n",
      "\n",
      "A good monsoon is likely to ensure adequate sowing and a bountiful harvest -- cooling inflation, which, at the retail level in April at 7.9%, is at a eight-year high(reports @jayashreenandi ) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_cluster(df, clusters, cluster_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Note on Evaluation\n",
    "\n",
    "There are really only three methods for evaluating topic models:\n",
    "\n",
    "1. Comparison with a \"ground truth\".  Where labelled datasets are available (for example the benchmark \"20newsgroups\" dataset frequently used to evaluate topic models) it is straightforward to measure correspondence.  This approach was not available for this model.\n",
    "2. Topic _coherence_ or _pereplexity_.  These are standard measures that describe the internal consistency of the found topics.  Both require that the approach taken yields a probabilistic model (and so are frequently used to evaluate modern Dirichlet-process based models) but will not be useful here.  Other methods (for example based on _information gain_) require that topics be represented as distributions of words.  Again, something not explicitly done here.\n",
    "3. Human judgement.  This is probably the most important method when building a topic model for a real-world use case.  How useful do humans find the results?  \n",
    "\n",
    "The algorithm in this workbook was developed and tested in conjunction with Public Affairs experts who were using the results to calculate the trends and flows of particular stories online.  Those individuals' judgement were the main evaluation criteria we used.\n",
    "\n",
    "Clearly I would like to tune the various hyperparameters in this algorithm to maximise some kind of measure of topic quality.  I'll post an update to the workbook when I think of one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring new values\n",
    "\n",
    "Finally, a quick example to show how new-arriving data can be scored.  The beauty of using a BERT model is that the wordpiece tokenisation step enables us to easily incorporate new documents containing out-of-vocabulary words (something that has traditionally stymied probabilistic and matrix-based topic-models that rely on word-tokenisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[1m\u001b[31mNEW DOCUMENT\u001b[0m\n",
      "\u001b[1mTory MP Lee Anderson says there food banks not needed in the UK\u001b[0m \n",
      "\n",
      "\u001b[1m\u001b[32mAssigned to Cluster [278] which has [21] members.\n",
      "\u001b[0m\n",
      "\u001b[4m\u001b[1m\u001b[31mSIMILAR DOCUMENTS\u001b[0m\n",
      "\u001b[34mDOC_ID: 79551\u001b[0m\n",
      "Completely out of touch: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "\u001b[34mDOC_ID: 27826\u001b[0m\n",
      "A thicky who can’t be bothered to research beyond their gut prejudice demonstrates fact in Mother of Parliaments, part 408.: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "\u001b[34mDOC_ID: 80315\u001b[0m\n",
      "Conservative members of English parliament are well known for preparing home cooked meals from “scratch.”: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "\u001b[34mDOC_ID: 18057\u001b[0m\n",
      "And we’re back to the “deserving” and “undeserving” poor… Bring back the Poor Laws then?: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "\u001b[34mDOC_ID: 72873\u001b[0m\n",
      "Caring Conservatives.: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "\u001b[34mDOC_ID: 27826\u001b[0m\n",
      "A thicky who can’t be bothered to research beyond their gut prejudice demonstrates fact in Mother of Parliaments, part 408.: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "\u001b[34mDOC_ID: 3069\u001b[0m\n",
      "30p Lee.#30pLee: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "\u001b[34mDOC_ID: 72226\u001b[0m\n",
      "Can't cook at all if you can't afford food: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "\u001b[34mDOC_ID: 78325\u001b[0m\n",
      "Clueless #tosser: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n",
      "\u001b[34mDOC_ID: 79551\u001b[0m\n",
      "Completely out of touch: \"Conservative MP Lee Anderson says there isn't a massive need for food banks in the UK, it’s just that people “canno… \" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "new = \"Tory MP Lee Anderson says there food banks not needed in the UK\"\n",
    "\n",
    "vec = model.encode(\n",
    "    new, \n",
    "    convert_to_numpy=True, \n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "def get_cluster_id(id):\n",
    "    try:\n",
    "        return G.nodes[id]['cluster_id']\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "dists = embedding.most_similar([vec], indexer=annoy_index)\n",
    "dists = [(doc_id, score) for doc_id, score in dists if score >= MIN_SIM_SCORE and score < 1.0]\n",
    "C = Counter([get_cluster_id(id) for id, _ in dists])\n",
    "cluster_id = C.most_common()[0][0]\n",
    "\n",
    "print(colored(\"NEW DOCUMENT\", 'red', attrs=['bold', 'underline']))\n",
    "print(colored(new, attrs=['bold']), '\\n')\n",
    "print(colored(f\"Assigned to Cluster [{cluster_id}] which has [{cluster_sizes[cluster_id]}] members.\\n\", 'green', attrs=['bold']))\n",
    "print(colored(\"SIMILAR DOCUMENTS\", 'red', attrs=['bold', 'underline']))\n",
    "\n",
    "cluster = clusters[cluster_id]\n",
    "sample = np.random.choice(cluster, min(10, len(cluster)))    \n",
    "\n",
    "for doc_id in sample:\n",
    "    print(colored(f\"DOC_ID: {doc_id}\", \"blue\"))\n",
    "    print(df.tweet_text.values[doc_id].replace('\\n', ''), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
